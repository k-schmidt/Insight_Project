# Design Patterns and MapReduce

## MapReduce History

MapReduce began as a Google paper in 2004 and shortly thereafter a free and open source software engineer named Doug Cutting started working on a MapReduce implementation to solve scalability in another project he was working on called Nutch.
Hadoop began as a project invested by Yahoo and split out to an Apache Foundation project.

## MapReduce and Hadoop Refresher
Hadoop MapReduce jobs are divided into a set of *map tasks* and *reduce tasks* that run in a distributed fashion on a cluster of computers.
The map tasks generally load, parse, transform, and filter data.
Each reduce task is responsible for handling a subset of the map task output.
The input to a MapReduce job is a set of files in the data store that are spread out over the *Hadoop Distributed File System (HDFS)*.


Map tasks:
- Record Reader
    - Translates an input split generated by input format into records.
    - Does not parse the record itself.
    - Passes data to the mapper in the form of a key/value pair.
        - Key is positional information
        - Value is the chunk of data that composes a record.
- Mapper
    - User-provided code is executed on each key/value pair from the record reader to produce zero or more new key/value pairs, called intermediate pairs.
        - Key is what the data will be grouped on.
        - Value is the information pertinent to the analysis in the reducer.
- Combiner
    - Groups data in the map phase.
    - Takes the intermediate keys from the mapper and applies a user-provided method to aggregate values in the small scope of that one mapper.
- Partitioner
    - Takes the intermediate key/value pairs from the mapper (or combiner if it is being used) and splits them up into shards, one shard per reducer.
    - Uses a hash function to distribute data to its correct reducer and ensures an even distribution to the number of reducers.
    - Partitioned data is written to the local file system for each map task and waits to be pulled by its respective reducer.


Reduce tasks:
- Shuffle
    - Takes the output files written by all of the partitioners and downloads them to the local machine in which the reducer is running.
- Sort
    - Group equivalent keys together so that their values can be iterated over easily in the reduce task.
- Reducer
    - Runs reduce function once per grouping.
    - Sends zero or more key/value pair to the final step, the output format.
- Output Format
    - Write the output of the reduce function to a file by a record writer.

## Hive and Pig
When you can, write your MapReduce in Pig or Hive.


## Average Example
The mapper creates two columns for the sum and count of each key.
This is what is output by the Combiner.
The reducer is then able to run an aggregation over all the key/values provided by the combiner.

## Filter Example
Filter does not require a reduce step - only mappers.
Filtering does not produce an aggregation.
Each record is looked at individually.

## Top 10 Example
In SQL, you might be inclined to sort your data set by the ranking value, then take the top K records from that.
This pattern utilizes both the mapper and the reducer.
The mappers will find their local top K, then all of the individual top K sets will compete for the final top K in the reducer.
We should expect K * M records coming into the reducer under one key where M is the number of map tasks.
This pattern only uses one reducer. We may need to pay attention to how many records the reducer is getting.
You may be able to further filter the dataset based on a set of assumptions.
Such as the top ten web page visits will need to be greater than 50,000.
This is one way of reducing the reducer bottleneck.


## Total Order Sorting
This pattern has two phases: an analyze phase that determines the ranges, and the order phase that actually sorts the data.


Analyze step:
- The mapper does a simple random sampling.
    - When dividing records, it outputs the sort key as its output key so that the data will show up sorted at the reducer.
- Determine the number of records in the total data set and figure out what percentage of records you'll need to analyze to make a reasonable sample.
- Only one reducer will be used here. This will collect the sort keys together into a sorted list.

Order step:
- Mapper extracts the sort key in the same way as the analyze step.
- Custom partitioner is uesd that loads up the partition file. Decides which reducer to send the data to.
- Reducer's job is simple. The shuffle and sort take care of the heavy lifting. The reduce function simply takes the values that have come in and outputs them.

## Join Patterns
Joins have a tendency to use a lot of network bandwith. Anything we can do to make the network transfer more efficient is worthwhile, and network optimizations are what differentiates these patterns.


## MapReduce
- key/value pair -> mapper -> [(k, v), (k, v), (k, v), ...]
- -> shuffle -> (k , [v, v, v, v, v, ...]) -> reducer -> [(k, v), (k, v), (k, v), ...]

### Word Count
- key is going to be a file, the value is going to be the text.
- The mapper is going to take the value (text) and create individual key/value pairs for each word.
- Shuffle - group the like keys. For example, all of the 'a's and 'the's are in one place.
- Produce lists of the counts that we get.
- If we couldn't get all of the values on one reducer then we could have multiple reduce steps.
- Finally, reducer does the aggregation
